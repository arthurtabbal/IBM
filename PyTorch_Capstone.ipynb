{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "4.1_resnet18_PyTorch.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arthurtabbal/IBM/blob/master/PyTorch_Capstone.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iY16clJ9Wy3I",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"http://cocl.us/pytorch_link_top\">\n",
        "    <img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/Pytochtop.png\" width=\"750\" alt=\"IBM Product \" />\n",
        "</a> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NRMZgYSWy3N",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/cc-logo-square.png\" width=\"200\" alt=\"cognitiveclass.ai logo\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWIYqNKhWy3Q",
        "colab_type": "text"
      },
      "source": [
        "<h1><h1>Pre-trained-Models with PyTorch </h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtwT6DIYWy3S",
        "colab_type": "text"
      },
      "source": [
        "In this lab, you will use pre-trained models to classify between the negative and positive samples; you will be provided with the dataset object. The particular pre-trained model will be resnet18; you will have three questions: \n",
        "<ul>\n",
        "<li>change the output layer</li>\n",
        "<li> train the model</li> \n",
        "<li>  identify  several  misclassified samples</li> \n",
        " </ul>\n",
        "You will take several screenshots of your work and share your notebook. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nI899sajWy3U",
        "colab_type": "text"
      },
      "source": [
        "<h2>Table of Contents</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEQZgssyWy3X",
        "colab_type": "text"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
        "\n",
        "\n",
        "<ul>\n",
        "    <li><a href=\"#download_data\"> Download Data</a></li>\n",
        "    <li><a href=\"#auxiliary\"> Imports and Auxiliary Functions </a></li>\n",
        "    <li><a href=\"#data_class\"> Dataset Class</a></li>\n",
        "    <li><a href=\"#Question_1\">Question 1</a></li>\n",
        "    <li><a href=\"#Question_2\">Question 2</a></li>\n",
        "    <li><a href=\"#Question_3\">Question 3</a></li>\n",
        "</ul>\n",
        "<p>Estimated Time Needed: <strong>120 min</strong></p>\n",
        " </div>\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsAuXykeWy3Z",
        "colab_type": "text"
      },
      "source": [
        "<h2 id=\"download_data\">Download Data</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCBjZWB-Wy3b",
        "colab_type": "text"
      },
      "source": [
        "Download the dataset and unzip the files in your data directory, unlike the other labs, all the data will be deleted after you close  the lab, this may take some time:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsLLr8e1Wy3d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "96d5120b-4ce6-4c3c-f688-c2510e9938c9"
      },
      "source": [
        "!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-20 23:12:33--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip\n",
            "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
            "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2598656062 (2.4G) [application/zip]\n",
            "Saving to: ‘Positive_tensors.zip’\n",
            "\n",
            "Positive_tensors.zi 100%[===================>]   2.42G  23.1MB/s    in 1m 48s  \n",
            "\n",
            "2020-05-20 23:14:22 (23.0 MB/s) - ‘Positive_tensors.zip’ saved [2598656062/2598656062]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBuuY9REWy3q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip -q Positive_tensors.zip "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oj_Czc4FWy32",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "39c738e3-5c8b-4ef3-cf9e-4444287d926e"
      },
      "source": [
        "! wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-20 23:19:00--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\n",
            "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
            "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2111408108 (2.0G) [application/zip]\n",
            "Saving to: ‘Negative_tensors.zip’\n",
            "\n",
            "Negative_tensors.zi 100%[===================>]   1.97G  26.6MB/s    in 74s     \n",
            "\n",
            "2020-05-20 23:20:14 (27.2 MB/s) - ‘Negative_tensors.zip’ saved [2111408108/2111408108]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dGpN7msWy4B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip -q Negative_tensors.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NV5TTltYWy4M",
        "colab_type": "text"
      },
      "source": [
        "We will install torchvision:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqegS2j1Wy4O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "5b49a950-f3f5-433b-b895-1f7b0accc1aa"
      },
      "source": [
        "!pip3 install torchvision"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.6.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.18.4)\n",
            "Requirement already satisfied: torch==1.5.0 in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.5.0+cu101)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.0->torchvision) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Begwj9sDWy4c",
        "colab_type": "text"
      },
      "source": [
        "<h2 id=\"auxiliary\">Imports and Auxiliary Functions</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeE1NIkuWy4g",
        "colab_type": "text"
      },
      "source": [
        "The following are the libraries we are going to use for this lab. The <code>torch.manual_seed()</code> is for forcing the random function to give the same number every time we try to recompile it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTxmkwwEWy4j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8b9090f7-988a-4535-a2b4-082683650713"
      },
      "source": [
        "# These are the libraries will be used for this lab.\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "import pandas\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import torch \n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import h5py\n",
        "import os\n",
        "import glob\n",
        "torch.manual_seed(0)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f0d963ec7b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naYBb7D0Wy4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib.pyplot import imshow\n",
        "import matplotlib.pylab as plt\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dm2KQRrFWy47",
        "colab_type": "text"
      },
      "source": [
        "<!--Empty Space for separating topics-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtXHnEcsWy49",
        "colab_type": "text"
      },
      "source": [
        "<h2 id=\"data_class\">Dataset Class</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mq1A_MJsWy4_",
        "colab_type": "text"
      },
      "source": [
        " This dataset class is essentially the same dataset you build in the previous section, but to speed things up, we are going to use tensors instead of jpeg images. Therefor for each iteration, you will skip the reshape step, conversion step to tensors and normalization step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKZf9rBvWy5B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c79c189d-6965-4dd6-aece-05257ceaa141"
      },
      "source": [
        "# Create your own dataset object\n",
        "\n",
        "class Dataset(Dataset):\n",
        "\n",
        "    # Constructor\n",
        "    def __init__(self,transform=None,train=True):\n",
        "        directory=\"\"\n",
        "        positive=\"Positive_tensors\"\n",
        "        negative='Negative_tensors'\n",
        "\n",
        "        positive_file_path=os.path.join(directory,positive)\n",
        "        negative_file_path=os.path.join(directory,negative)\n",
        "        positive_files=[os.path.join(positive_file_path,file) for file in os.listdir(positive_file_path) if file.endswith(\".pt\")]\n",
        "        negative_files=[os.path.join(negative_file_path,file) for file in os.listdir(negative_file_path) if file.endswith(\".pt\")]\n",
        "        number_of_samples=len(positive_files)+len(negative_files)\n",
        "        self.all_files=[None]*number_of_samples\n",
        "        self.all_files[::2]=positive_files\n",
        "        self.all_files[1::2]=negative_files \n",
        "        # The transform is goint to be used on image\n",
        "        self.transform = transform\n",
        "        #torch.LongTensor\n",
        "        self.Y=torch.zeros([number_of_samples]).type(torch.LongTensor)\n",
        "        self.Y[::2]=1\n",
        "        self.Y[1::2]=0\n",
        "        \n",
        "        if train:\n",
        "            self.all_files=self.all_files[0:30000]\n",
        "            self.Y=self.Y[0:30000]\n",
        "            self.len=len(self.all_files)\n",
        "        else:\n",
        "            self.all_files=self.all_files[30000:]\n",
        "            self.Y=self.Y[30000:]\n",
        "            self.len=len(self.all_files)     \n",
        "       \n",
        "    # Get the length\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "    \n",
        "    # Getter\n",
        "    def __getitem__(self, idx):\n",
        "               \n",
        "        image=torch.load(self.all_files[idx])\n",
        "        y=self.Y[idx]\n",
        "                  \n",
        "        # If there is any transform method, apply it onto the image\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, y\n",
        "    \n",
        "print(\"done\")"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIU8msGOWy5M",
        "colab_type": "text"
      },
      "source": [
        "We create two dataset objects, one for the training data and one for the validation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ai7vEdAZWy5P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7c308cfd-5eaf-4ebd-f666-ec72073f6195"
      },
      "source": [
        "train_dataset = Dataset(train=True)\n",
        "validation_dataset = Dataset(train=False)\n",
        "print(\"done\")"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8UoHqxzWy5i",
        "colab_type": "text"
      },
      "source": [
        "<h2 id=\"Question_1\">Question 1</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpVFjgjaWy5k",
        "colab_type": "text"
      },
      "source": [
        "<b>Prepare a pre-trained resnet18 model :</b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbOhKR1uWy5m",
        "colab_type": "text"
      },
      "source": [
        "<b>Step 1</b>: Load the pre-trained model <code>resnet18</code> Set the parameter <code>pretrained</code> to true:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixWzu0UwWy5o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step 1: Load the pre-trained model resnet18\n",
        "\n",
        "model = models.resnet18(pretrained=True)\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "\n",
        "# Type your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9qRrkshWy5y",
        "colab_type": "text"
      },
      "source": [
        "<b>Step 2</b>: Set the attribute <code>requires_grad</code> to <code>False</code>. As a result, the parameters will not be affected by training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFI2eVjnWy50",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step 2: Set the parameter cannot be trained for the pre-trained model\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "# Type your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pL0sQdaXWy6C",
        "colab_type": "text"
      },
      "source": [
        "<code>resnet18</code> is used to classify 1000 different objects; as a result, the last layer has 1000 outputs.  The 512 inputs come from the fact that the previously hidden layer has 512 outputs. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PphRyG5eWy6D",
        "colab_type": "text"
      },
      "source": [
        "<b>Step 3</b>: Replace the output layer <code>model.fc</code> of the neural network with a <code>nn.Linear</code> object, to classify 2 different classes. For the parameters <code>in_features </code> remember the last hidden layer has 512 neurons."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iM1llf7XWy6F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fc = nn.Linear(512,2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzR_AesrWy6P",
        "colab_type": "text"
      },
      "source": [
        "Print out the model in order to show whether you get the correct answer.<br> <b>(Your peer reviewer is going to mark based on what you print here.)</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmcIUMRmWy6Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6ae00dce-6207-4e45-c553-543f5d0cc7e5"
      },
      "source": [
        "print(model)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnrz-sskWy6c",
        "colab_type": "text"
      },
      "source": [
        "<h2 id=\"Question_2\">Question 2: Train the Model</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIO-Zg94Wy6d",
        "colab_type": "text"
      },
      "source": [
        "In this question you will train your, model:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLHa6X0DWy6f",
        "colab_type": "text"
      },
      "source": [
        "<b>Step 1</b>: Create a cross entropy criterion function "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymo9xjNNWy6i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step 1: Create the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Type your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmR6ZP-kWy6r",
        "colab_type": "text"
      },
      "source": [
        "<b>Step 2</b>: Create a training loader and validation loader object, the batch size should have 100 samples each."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGZO4IoFWy6t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100)\n",
        "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrmx5fTxWy61",
        "colab_type": "text"
      },
      "source": [
        "<b>Step 3</b>: Use the following optimizer to minimize the loss "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzJg4VFbWy63",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = torch.optim.Adam([parameters  for parameters in model.parameters() if parameters.requires_grad],lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsBiYrSjWy7A",
        "colab_type": "text"
      },
      "source": [
        "<!--Empty Space for separating topics-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McGrBRU7Wy7B",
        "colab_type": "text"
      },
      "source": [
        "**Complete the following code to calculate  the accuracy on the validation data for one epoch; this should take about 45 minutes. Make sure you calculate the accuracy on the validation data.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dfz0cldWy7D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c03512b9-4916-4806-808a-8dd582c999ae"
      },
      "source": [
        "n_epochs=1\n",
        "loss_list=[]\n",
        "accuracy_list=[]\n",
        "correct=0\n",
        "N_test=len(validation_dataset)\n",
        "N_train=len(train_dataset)\n",
        "start_time = time.time()\n",
        "#n_epochs\n",
        "\n",
        "Loss=0\n",
        "start_time = time.time()\n",
        "current = 0\n",
        "for epoch in range(n_epochs):\n",
        "    for x, y in train_loader:\n",
        "        #set model to train\n",
        "        model.train()\n",
        "        #clear gradien\n",
        "        optimizer.zero_grad()\n",
        "        #make a prediction\n",
        "        yhat = model(x)\n",
        "        # calculate loss\n",
        "        loss = criterion(yhat, y)\n",
        "        # calculate gradients of parameters\n",
        "        loss.backward()\n",
        "        # update parameters \n",
        "        optimizer.step()\n",
        "        loss_list.append(loss.data)\n",
        "        current += 1\n",
        "        print(current)\n",
        "    correct=0\n",
        "    for x_test, y_test in validation_loader:\n",
        "        # set model to eval\n",
        "        model.eval()\n",
        "        #make a prediction \n",
        "        z = model(x_test)\n",
        "        #find max\n",
        "        _, yhat = torch.max(z.data, 1)\n",
        "        #Calculate misclassified  samples in mini-batch \n",
        "        #hint +=(yhat==y_test).sum().item()\n",
        "        correct += (yhat==y_test).sum().item()\n",
        "        current += 1\n",
        "        print(current)\n",
        "   \n",
        "    accuracy=correct/N_test\n",
        "    accuracy_list.append(accuracy)\n",
        "\n"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "271\n",
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "276\n",
            "277\n",
            "278\n",
            "279\n",
            "280\n",
            "281\n",
            "282\n",
            "283\n",
            "284\n",
            "285\n",
            "286\n",
            "287\n",
            "288\n",
            "289\n",
            "290\n",
            "291\n",
            "292\n",
            "293\n",
            "294\n",
            "295\n",
            "296\n",
            "297\n",
            "298\n",
            "299\n",
            "300\n",
            "301\n",
            "302\n",
            "303\n",
            "304\n",
            "305\n",
            "306\n",
            "307\n",
            "308\n",
            "309\n",
            "310\n",
            "311\n",
            "312\n",
            "313\n",
            "314\n",
            "315\n",
            "316\n",
            "317\n",
            "318\n",
            "319\n",
            "320\n",
            "321\n",
            "322\n",
            "323\n",
            "324\n",
            "325\n",
            "326\n",
            "327\n",
            "328\n",
            "329\n",
            "330\n",
            "331\n",
            "332\n",
            "333\n",
            "334\n",
            "335\n",
            "336\n",
            "337\n",
            "338\n",
            "339\n",
            "340\n",
            "341\n",
            "342\n",
            "343\n",
            "344\n",
            "345\n",
            "346\n",
            "347\n",
            "348\n",
            "349\n",
            "350\n",
            "351\n",
            "352\n",
            "353\n",
            "354\n",
            "355\n",
            "356\n",
            "357\n",
            "358\n",
            "359\n",
            "360\n",
            "361\n",
            "362\n",
            "363\n",
            "364\n",
            "365\n",
            "366\n",
            "367\n",
            "368\n",
            "369\n",
            "370\n",
            "371\n",
            "372\n",
            "373\n",
            "374\n",
            "375\n",
            "376\n",
            "377\n",
            "378\n",
            "379\n",
            "380\n",
            "381\n",
            "382\n",
            "383\n",
            "384\n",
            "385\n",
            "386\n",
            "387\n",
            "388\n",
            "389\n",
            "390\n",
            "391\n",
            "392\n",
            "393\n",
            "394\n",
            "395\n",
            "396\n",
            "397\n",
            "398\n",
            "399\n",
            "400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjvswyUFWy7N",
        "colab_type": "text"
      },
      "source": [
        "<b>Print out the Accuracy and plot the loss stored in the list <code>loss_list</code> for every iteration and take a screen shot.</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBQVNJcYWy7P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "20859561-571e-4e45-d916-35324e04d0d2"
      },
      "source": [
        "accuracy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a25c38b2cb47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'accuracy' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfDDsmzSWy7Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "edf457e5-75f8-4613-ffb2-8d09059e6fad"
      },
      "source": [
        "plt.plot(loss_list)\n",
        "plt.xlabel(\"iteration\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()\n"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smrrvBDHWy7m",
        "colab_type": "text"
      },
      "source": [
        "<h2 id=\"Question_3\">Question 3:Find the misclassified samples</h2> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMh1oZV8Wy7n",
        "colab_type": "text"
      },
      "source": [
        "<b>Identify the first four misclassified samples using the validation data:</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHH9QqDsWy7p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2n0wgNoWy7y",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/share-notebooks.html\"> CLICK HERE </a> Click here to see how to share your notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMYnxwYUWy7z",
        "colab_type": "text"
      },
      "source": [
        "<h2>About the Authors:</h2> \n",
        "\n",
        "<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iKfAU4-Wy71",
        "colab_type": "text"
      },
      "source": [
        "Copyright &copy; 2018 <a href=\"cognitiveclass.ai?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu\">cognitiveclass.ai</a>. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/\">MIT License</a>."
      ]
    }
  ]
}