{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "iY16clJ9Wy3I"
            },
            "source": "<a href=\"http://cocl.us/pytorch_link_top\">\n    <img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/Pytochtop.png\" width=\"750\" alt=\"IBM Product \" />\n</a> "
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "7NRMZgYSWy3N"
            },
            "source": "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/cc-logo-square.png\" width=\"200\" alt=\"cognitiveclass.ai logo\" />"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "OWIYqNKhWy3Q"
            },
            "source": "<h1><h1>Pre-trained-Models with PyTorch </h1>"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "xtwT6DIYWy3S"
            },
            "source": "In this lab, you will use pre-trained models to classify between the negative and positive samples; you will be provided with the dataset object. The particular pre-trained model will be resnet18; you will have three questions: \n<ul>\n<li>change the output layer</li>\n<li> train the model</li> \n<li>  identify  several  misclassified samples</li> \n </ul>\nYou will take several screenshots of your work and share your notebook. "
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "nI899sajWy3U"
            },
            "source": "<h2>Table of Contents</h2>"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "lEQZgssyWy3X"
            },
            "source": "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n\n\n<ul>\n    <li><a href=\"#download_data\"> Download Data</a></li>\n    <li><a href=\"#auxiliary\"> Imports and Auxiliary Functions </a></li>\n    <li><a href=\"#data_class\"> Dataset Class</a></li>\n    <li><a href=\"#Question_1\">Question 1</a></li>\n    <li><a href=\"#Question_2\">Question 2</a></li>\n    <li><a href=\"#Question_3\">Question 3</a></li>\n</ul>\n<p>Estimated Time Needed: <strong>120 min</strong></p>\n </div>\n<hr>"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "MsAuXykeWy3Z"
            },
            "source": "<h2 id=\"download_data\">Download Data</h2>"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "kCBjZWB-Wy3b"
            },
            "source": "Download the dataset and unzip the files in your data directory, unlike the other labs, all the data will be deleted after you close  the lab, this may take some time:"
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 204
                },
                "colab_type": "code",
                "id": "NsLLr8e1Wy3d",
                "outputId": "96d5120b-4ce6-4c3c-f688-c2510e9938c9"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "--2020-05-21 13:55:31--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip\nResolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\nConnecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2598656062 (2.4G) [application/zip]\nSaving to: \u2018Positive_tensors.zip\u2019\n\n100%[====================================>] 2,598,656,062 45.1MB/s   in 57s    \n\n2020-05-21 13:56:28 (43.8 MB/s) - \u2018Positive_tensors.zip\u2019 saved [2598656062/2598656062]\n\n"
                }
            ],
            "source": "!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip "
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "colab": {},
                "colab_type": "code",
                "id": "oBuuY9REWy3q"
            },
            "outputs": [],
            "source": "!unzip -q Positive_tensors.zip "
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 204
                },
                "colab_type": "code",
                "id": "oj_Czc4FWy32",
                "outputId": "39c738e3-5c8b-4ef3-cf9e-4444287d926e"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "--2020-05-21 13:58:04--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\nResolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\nConnecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2111408108 (2.0G) [application/zip]\nSaving to: \u2018Negative_tensors.zip\u2019\n\n100%[====================================>] 2,111,408,108 43.9MB/s   in 46s    \n\n2020-05-21 13:58:50 (44.2 MB/s) - \u2018Negative_tensors.zip\u2019 saved [2111408108/2111408108]\n\n"
                }
            ],
            "source": "!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "colab": {},
                "colab_type": "code",
                "id": "0dGpN7msWy4B"
            },
            "outputs": [],
            "source": "!unzip -q Negative_tensors.zip"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "NV5TTltYWy4M"
            },
            "source": "We will install torchvision:"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 102
                },
                "colab_type": "code",
                "id": "vqegS2j1Wy4O",
                "outputId": "5b49a950-f3f5-433b-b895-1f7b0accc1aa"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Collecting torchvision\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/51/aa2770a70f612ce9a2fc7da3a1a93f9ecf8746788256fed6b691f9b31ca9/torchvision-0.6.0-cp36-cp36m-manylinux1_x86_64.whl (6.6MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.6MB 5.9MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: pillow>=4.1.1 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from torchvision) (5.4.1)\nRequirement already satisfied: numpy in /opt/conda/envs/Python36/lib/python3.6/site-packages (from torchvision) (1.15.4)\nCollecting torch==1.5.0 (from torchvision)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/70/54e9fb010fe1547bc4774716f11ececb81ae5b306c05f090f4461ee13205/torch-1.5.0-cp36-cp36m-manylinux1_x86_64.whl (752.0MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 752.0MB 16kB/s s eta 0:00:014MB 31.9MB/s eta 0:00:23                         | 85.1MB 28.7MB/s eta 0:00:24     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a                       | 204.3MB 38.5MB/s eta 0:00:15     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                  | 314.4MB 33.0MB/s eta 0:00:14\ufffd\u2588\u2588\u258a| 745.6MB 41.7MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: future in /opt/conda/envs/Python36/lib/python3.6/site-packages (from torch==1.5.0->torchvision) (0.17.1)\nInstalling collected packages: torch, torchvision\nSuccessfully installed torch-1.5.0 torchvision-0.6.0\n"
                }
            ],
            "source": "!pip install torchvision"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "Begwj9sDWy4c"
            },
            "source": "<h2 id=\"auxiliary\">Imports and Auxiliary Functions</h2>"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "WeE1NIkuWy4g"
            },
            "source": "The following are the libraries we are going to use for this lab. The <code>torch.manual_seed()</code> is for forcing the random function to give the same number every time we try to recompile it."
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 34
                },
                "colab_type": "code",
                "id": "BTxmkwwEWy4j",
                "outputId": "8b9090f7-988a-4535-a2b4-082683650713"
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "<torch._C.Generator at 0x7f52862b69b0>"
                    },
                    "execution_count": 8,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# These are the libraries will be used for this lab.\nimport torchvision.models as models\nfrom PIL import Image\nimport pandas\nfrom torchvision import transforms\nimport torch.nn as nn\nimport time\nimport torch \nimport matplotlib.pylab as plt\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nimport h5py\nimport os\nimport glob\ntorch.manual_seed(0)"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {
                "colab": {},
                "colab_type": "code",
                "id": "naYBb7D0Wy4x"
            },
            "outputs": [],
            "source": "from matplotlib.pyplot import imshow\nimport matplotlib.pylab as plt\nfrom PIL import Image\nimport pandas as pd\nimport os"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "Dm2KQRrFWy47"
            },
            "source": "<!--Empty Space for separating topics-->"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "RtXHnEcsWy49"
            },
            "source": "<h2 id=\"data_class\">Dataset Class</h2>"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "mq1A_MJsWy4_"
            },
            "source": " This dataset class is essentially the same dataset you build in the previous section, but to speed things up, we are going to use tensors instead of jpeg images. Therefor for each iteration, you will skip the reshape step, conversion step to tensors and normalization step."
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 34
                },
                "colab_type": "code",
                "id": "kKZf9rBvWy5B",
                "outputId": "c79c189d-6965-4dd6-aece-05257ceaa141"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "done\n"
                }
            ],
            "source": "# Create your own dataset object\n\nclass Dataset(Dataset):\n\n    # Constructor\n    def __init__(self,transform=None,train=True):\n        directory=\"\"\n        positive=\"Positive_tensors\"\n        negative='Negative_tensors'\n\n        positive_file_path=os.path.join(directory,positive)\n        negative_file_path=os.path.join(directory,negative)\n        positive_files=[os.path.join(positive_file_path,file) for file in os.listdir(positive_file_path) if file.endswith(\".pt\")]\n        negative_files=[os.path.join(negative_file_path,file) for file in os.listdir(negative_file_path) if file.endswith(\".pt\")]\n        number_of_samples=len(positive_files)+len(negative_files)\n        self.all_files=[None]*number_of_samples\n        self.all_files[::2]=positive_files\n        self.all_files[1::2]=negative_files \n        # The transform is goint to be used on image\n        self.transform = transform\n        #torch.LongTensor\n        self.Y=torch.zeros([number_of_samples]).type(torch.LongTensor)\n        self.Y[::2]=1\n        self.Y[1::2]=0\n        \n        if train:\n            self.all_files=self.all_files[0:30000]\n            self.Y=self.Y[0:30000]\n            self.len=len(self.all_files)\n        else:\n            self.all_files=self.all_files[30000:]\n            self.Y=self.Y[30000:]\n            self.len=len(self.all_files)     \n       \n    # Get the length\n    def __len__(self):\n        return self.len\n    \n    # Getter\n    def __getitem__(self, idx):\n               \n        image=torch.load(self.all_files[idx])\n        y=self.Y[idx]\n                  \n        # If there is any transform method, apply it onto the image\n        if self.transform:\n            image = self.transform(image)\n\n        return image, y\n    \nprint(\"done\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "FIU8msGOWy5M"
            },
            "source": "We create two dataset objects, one for the training data and one for the validation data."
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 34
                },
                "colab_type": "code",
                "id": "ai7vEdAZWy5P",
                "outputId": "7c308cfd-5eaf-4ebd-f666-ec72073f6195"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "done\n"
                }
            ],
            "source": "train_dataset = Dataset(train=True)\nvalidation_dataset = Dataset(train=False)\nprint(\"done\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "f8UoHqxzWy5i"
            },
            "source": "<h2 id=\"Question_1\">Question 1</h2>"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "TpVFjgjaWy5k"
            },
            "source": "<b>Prepare a pre-trained resnet18 model :</b>"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "hbOhKR1uWy5m"
            },
            "source": "<b>Step 1</b>: Load the pre-trained model <code>resnet18</code> Set the parameter <code>pretrained</code> to true:"
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {
                "colab": {},
                "colab_type": "code",
                "id": "ixWzu0UwWy5o"
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /home/dsxuser/.cache/torch/checkpoints/resnet18-5c106cde.pth\n"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "1865b4184fa34534b956a6ffdb489732",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": "HBox(children=(IntProgress(value=0, max=46827520), HTML(value='')))"
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\n"
                }
            ],
            "source": "# Step 1: Load the pre-trained model resnet18\n\nmodel = models.resnet18(pretrained=True)\n\n# Type your code here"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "Z9qRrkshWy5y"
            },
            "source": "<b>Step 2</b>: Set the attribute <code>requires_grad</code> to <code>False</code>. As a result, the parameters will not be affected by training."
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {
                "colab": {},
                "colab_type": "code",
                "id": "DFI2eVjnWy50"
            },
            "outputs": [],
            "source": "# Step 2: Set the parameter cannot be trained for the pre-trained model\nfor param in model.parameters():\n    param.requires_grad = False\n# Type your code here"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "pL0sQdaXWy6C"
            },
            "source": "<code>resnet18</code> is used to classify 1000 different objects; as a result, the last layer has 1000 outputs.  The 512 inputs come from the fact that the previously hidden layer has 512 outputs. "
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "PphRyG5eWy6D"
            },
            "source": "<b>Step 3</b>: Replace the output layer <code>model.fc</code> of the neural network with a <code>nn.Linear</code> object, to classify 2 different classes. For the parameters <code>in_features </code> remember the last hidden layer has 512 neurons."
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {
                "colab": {},
                "colab_type": "code",
                "id": "iM1llf7XWy6F"
            },
            "outputs": [],
            "source": "model.fc = nn.Linear(512,2)"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "rzR_AesrWy6P"
            },
            "source": "Print out the model in order to show whether you get the correct answer.<br> <b>(Your peer reviewer is going to mark based on what you print here.)</b>"
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 1000
                },
                "colab_type": "code",
                "id": "cmcIUMRmWy6Q",
                "outputId": "6ae00dce-6207-4e45-c553-543f5d0cc7e5"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=2, bias=True)\n)\n"
                }
            ],
            "source": "print(model)"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "hnrz-sskWy6c"
            },
            "source": "<h2 id=\"Question_2\">Question 2: Train the Model</h2>"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "pIO-Zg94Wy6d"
            },
            "source": "In this question you will train your, model:"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "SLHa6X0DWy6f"
            },
            "source": "<b>Step 1</b>: Create a cross entropy criterion function "
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {
                "colab": {},
                "colab_type": "code",
                "id": "ymo9xjNNWy6i"
            },
            "outputs": [],
            "source": "# Step 1: Create the loss function\ncriterion = nn.CrossEntropyLoss()\n# Type your code here"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "XmR6ZP-kWy6r"
            },
            "source": "<b>Step 2</b>: Create a training loader and validation loader object, the batch size should have 100 samples each."
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {
                "colab": {},
                "colab_type": "code",
                "id": "pGZO4IoFWy6t"
            },
            "outputs": [],
            "source": "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100)\nvalidation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=100)"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "jrmx5fTxWy61"
            },
            "source": "<b>Step 3</b>: Use the following optimizer to minimize the loss "
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {
                "colab": {},
                "colab_type": "code",
                "id": "qzJg4VFbWy63"
            },
            "outputs": [],
            "source": "optimizer = torch.optim.Adam([parameters  for parameters in model.parameters() if parameters.requires_grad],lr=0.001)"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "dsBiYrSjWy7A"
            },
            "source": "<!--Empty Space for separating topics-->"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "McGrBRU7Wy7B"
            },
            "source": "**Complete the following code to calculate  the accuracy on the validation data for one epoch; this should take about 45 minutes. Make sure you calculate the accuracy on the validation data.**"
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 1000
                },
                "colab_type": "code",
                "id": "7dfz0cldWy7D",
                "outputId": "c03512b9-4916-4806-808a-8dd582c999ae"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n"
                }
            ],
            "source": "n_epochs=1\nloss_list=[]\naccuracy_list=[]\ncorrect=0\nN_test=len(validation_dataset)\nN_train=len(train_dataset)\nstart_time = time.time()\n#n_epochs\n\nLoss=0\nstart_time = time.time()\ncurrent = 0\nfor epoch in range(n_epochs):\n    for x, y in train_loader:\n        #set model to train\n        model.train()\n        #clear gradien\n        optimizer.zero_grad()\n        #make a prediction\n        yhat = model(x)\n        # calculate loss\n        loss = criterion(yhat, y)\n        # calculate gradients of parameters\n        loss.backward()\n        # update parameters \n        optimizer.step()\n        loss_list.append(loss.data)\n        \n        #for tracking progress\n        current += 1\n        print(current)\n    correct=0\n    for x_test, y_test in validation_loader:\n        # set model to eval\n        model.eval()\n        #make a prediction \n        z = model(x_test)\n        #find max\n        _, yhat = torch.max(z.data, 1)\n        #Calculate misclassified  samples in mini-batch \n        #hint +=(yhat==y_test).sum().item()\n        correct += (yhat==y_test).sum().item()\n        \n        #for tracking progress\n        current += 1\n        print(current)\n   \n    accuracy=correct/N_test\n    accuracy_list.append(accuracy)\n\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "LjvswyUFWy7N"
            },
            "source": "<b>Print out the Accuracy and plot the loss stored in the list <code>loss_list</code> for every iteration and take a screen shot.</b>"
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 163
                },
                "colab_type": "code",
                "id": "zBQVNJcYWy7P",
                "outputId": "20859561-571e-4e45-d916-35324e04d0d2"
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "0.9948"
                    },
                    "execution_count": 20,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "accuracy"
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 279
                },
                "colab_type": "code",
                "id": "HfDDsmzSWy7Y",
                "outputId": "edf457e5-75f8-4613-ffb2-8d09059e6fad"
            },
            "outputs": [
                {
                    "data": {
                        "image/png": "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\n",
                        "text/plain": "<Figure size 432x288 with 1 Axes>"
                    },
                    "metadata": {
                        "needs_background": "light"
                    },
                    "output_type": "display_data"
                }
            ],
            "source": "plt.plot(loss_list)\nplt.xlabel(\"iteration\")\nplt.ylabel(\"loss\")\nplt.show()\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "smrrvBDHWy7m"
            },
            "source": "<h2 id=\"Question_3\">Question 3:Find the misclassified samples</h2> "
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "JMh1oZV8Wy7n"
            },
            "source": "<b>Identify the first four misclassified samples using the validation data:</b>"
        },
        {
            "cell_type": "code",
            "execution_count": 63,
            "metadata": {
                "colab": {},
                "colab_type": "code",
                "id": "AHH9QqDsWy7p"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Misclassified Sample 75 :  Predicted Value = 1  Actual Value = 0\nMisclassified Sample 412 :  Predicted Value = 0  Actual Value = 1\nMisclassified Sample 597 :  Predicted Value = 1  Actual Value = 0\nMisclassified Sample 598 :  Predicted Value = 0  Actual Value = 1\n"
                }
            ],
            "source": "count = 0\nbatch = 0\nfor x_test, y_test in validation_loader:\n    z = model(x_test)\n    _, y_hat = torch.max(z.data, 1)\n    if count == 4:\n        break\n    for i in range(len(y_hat)):\n        if y_test[i] != y_hat[i]:\n            count += 1\n            print('Misclassified Sample {} :'.format(i + 100*batch),\n                  ' Predicted Value = {}'.format(y_hat[i]),\n                  ' Actual Value = {}'.format(y_test[i]))\n    batch += 1\n\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "r2n0wgNoWy7y"
            },
            "source": "<a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/share-notebooks.html\"> CLICK HERE </a> Click here to see how to share your notebook."
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "fMYnxwYUWy7z"
            },
            "source": "<h2>About the Authors:</h2> \n\n<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD."
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "5iKfAU4-Wy71"
            },
            "source": "Copyright &copy; 2018 <a href=\"cognitiveclass.ai?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu\">cognitiveclass.ai</a>. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/\">MIT License</a>."
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "name": "Copy of 4.1_resnet18_PyTorch.ipynb",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}